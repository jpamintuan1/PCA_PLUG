{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "252/252 [==============================] - 0s 513us/step - loss: 0.1048\n",
      "Epoch 2/20\n",
      "252/252 [==============================] - 0s 495us/step - loss: 0.0606\n",
      "Epoch 3/20\n",
      "252/252 [==============================] - 0s 481us/step - loss: 0.0414\n",
      "Epoch 4/20\n",
      "252/252 [==============================] - 0s 446us/step - loss: 0.0333\n",
      "Epoch 5/20\n",
      "252/252 [==============================] - 0s 450us/step - loss: 0.0292\n",
      "Epoch 6/20\n",
      "252/252 [==============================] - 0s 513us/step - loss: 0.0268\n",
      "Epoch 7/20\n",
      "252/252 [==============================] - 0s 448us/step - loss: 0.0250\n",
      "Epoch 8/20\n",
      "252/252 [==============================] - 0s 469us/step - loss: 0.0236\n",
      "Epoch 9/20\n",
      "252/252 [==============================] - 0s 486us/step - loss: 0.0222\n",
      "Epoch 10/20\n",
      "252/252 [==============================] - 0s 442us/step - loss: 0.0209\n",
      "Epoch 11/20\n",
      "252/252 [==============================] - 0s 470us/step - loss: 0.0197\n",
      "Epoch 12/20\n",
      "252/252 [==============================] - 0s 460us/step - loss: 0.0185\n",
      "Epoch 13/20\n",
      "252/252 [==============================] - 0s 446us/step - loss: 0.0173\n",
      "Epoch 14/20\n",
      "252/252 [==============================] - 0s 460us/step - loss: 0.0162\n",
      "Epoch 15/20\n",
      "252/252 [==============================] - 0s 447us/step - loss: 0.0151\n",
      "Epoch 16/20\n",
      "252/252 [==============================] - 0s 466us/step - loss: 0.0140\n",
      "Epoch 17/20\n",
      "252/252 [==============================] - 0s 443us/step - loss: 0.0129\n",
      "Epoch 18/20\n",
      "252/252 [==============================] - 0s 452us/step - loss: 0.0119\n",
      "Epoch 19/20\n",
      "252/252 [==============================] - 0s 440us/step - loss: 0.0110\n",
      "Epoch 20/20\n",
      "252/252 [==============================] - 0s 517us/step - loss: 0.0101\n",
      "1/8 [==>...........................] - ETA: 0s - loss: 0.0959WARNING:tensorflow:Callbacks method `on_test_batch_end` is slow compared to the batch time (batch time: 0.0000s vs `on_test_batch_end` time: 0.0010s). Check your callbacks.\n",
      "8/8 [==============================] - 0s 623us/step - loss: 0.0146\n",
      "test loss: 0.014573941007256508\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "from pandas_datareader import data as pdr\n",
    "\n",
    "\n",
    "# Load data\n",
    "data = pd.read_csv('PLUG.csv', sep=',', engine='python')\n",
    "\n",
    "assets = data.columns.values[1:].tolist()\n",
    "data = data.iloc[:, 1:]\n",
    "\n",
    "# Load index\n",
    "index = pd.read_csv('TQQQ.csv', sep=',', engine='python')\n",
    "index = index.iloc[-data.values.shape[0]:, 1:]\n",
    "\n",
    "\n",
    "# Normalize data\n",
    "scaler = MinMaxScaler([0.1,0.9])\n",
    "data_X = scaler.fit_transform(data)\n",
    "scaler_index = MinMaxScaler([0.1,0.9])\n",
    "index = scaler_index.fit_transform(index)\n",
    "\n",
    "# Number of components\n",
    "N_COMPONENTS = 3\n",
    "\n",
    "## Autoencoder - Keras\n",
    "# Network hyperparameters\n",
    "n_inputs = len(assets)\n",
    "n_core = N_COMPONENTS\n",
    "n_outputs = n_inputs\n",
    "\n",
    "# Create model\n",
    "input = Input(shape=(n_inputs,))\n",
    "# Encoder\n",
    "encoded = Dense(n_core, activation='sigmoid')(input)\n",
    "# Decoder\n",
    "decoded = Dense(n_outputs, activation='sigmoid')(encoded)\n",
    "\n",
    "# define model\n",
    "autoencoder = Model(input, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Testing in-sample\n",
    "X_train = data_X\n",
    "X_test = data_X\n",
    "\n",
    "# Training parameters\n",
    "epochs = 20\n",
    "\n",
    "# Fit the model\n",
    "history = autoencoder.fit(X_train,\\\n",
    "                          X_train,\\\n",
    "                          epochs=epochs,\\\n",
    "                          batch_size=1,\\\n",
    "                          shuffle=True,\\\n",
    "                          verbose=1)\n",
    "\n",
    "# Make AE predictions\n",
    "y_pred_AE_keras = autoencoder.predict(X_test)\n",
    "\n",
    "print('test loss: '+str(autoencoder.evaluate(y_pred_AE_keras, X_test)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\josep\\anaconda3\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.utils import shuffle\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "n_inputs = len(assets)\n",
    "n_core = N_COMPONENTS\n",
    "n_outputs = n_inputs\n",
    "\n",
    "initializer = tf.initializers.glorot_normal()\n",
    "w1 = tf.Variable(initializer([n_inputs, n_core]))\n",
    "w2 = tf.transpose(w1)\n",
    "b1 = tf.Variable(tf.zeros([n_core]))\n",
    "b2 = tf.Variable(tf.zeros([n_outputs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Building the encoder\n",
    "def encoder(x):\n",
    "    return tf.nn.sigmoid(tf.add(tf.matmul(x, w1), b1))\n",
    "\n",
    "# Building the decoder\n",
    "def decoder(x):\n",
    "    return tf.nn.sigmoid(tf.add(tf.matmul(x, w2), b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 -> Loss: 0.001994\n",
      "Epoch: 1 -> Loss: 0.003119\n",
      "Epoch: 2 -> Loss: 0.001483\n",
      "Epoch: 3 -> Loss: 0.004482\n",
      "Epoch: 4 -> Loss: 0.001497\n",
      "Epoch: 5 -> Loss: 0.000779\n",
      "Epoch: 6 -> Loss: 0.000213\n",
      "Epoch: 7 -> Loss: 0.001107\n",
      "Epoch: 8 -> Loss: 0.000609\n",
      "Epoch: 9 -> Loss: 0.000599\n",
      "Epoch: 10 -> Loss: 0.000247\n",
      "Epoch: 11 -> Loss: 0.000061\n",
      "Epoch: 12 -> Loss: 0.004134\n",
      "Epoch: 13 -> Loss: 0.000614\n",
      "Epoch: 14 -> Loss: 0.000257\n",
      "Epoch: 15 -> Loss: 0.000077\n",
      "Epoch: 16 -> Loss: 0.000862\n",
      "Epoch: 17 -> Loss: 0.000181\n",
      "Epoch: 18 -> Loss: 0.000083\n",
      "Epoch: 19 -> Loss: 0.000160\n",
      "Test Error: 0.000390\n"
     ]
    }
   ],
   "source": [
    "X = tf.placeholder(\"float\", [None, n_inputs])\n",
    "Y = tf.placeholder(\"float\", [None, n_inputs])\n",
    "# Construct model\n",
    "encoder_op = encoder(X)\n",
    "decoder_op = decoder(encoder_op)\n",
    "# Prediction\n",
    "y_pred = decoder_op\n",
    "y_true = X\n",
    "\n",
    "\n",
    "# Training parameters\n",
    "lr = 0.01\n",
    "epochs = 20\n",
    "batch_size = 1\n",
    "\n",
    "\n",
    "mse = tf.losses.mean_squared_error(y_true, y_pred)\n",
    "optimizer = tf.train.AdamOptimizer(lr).minimize(mse)\n",
    "\n",
    "# Start Training\n",
    "# Start a new TF session\n",
    "with tf.Session() as sess:\n",
    "    # Initialize the network\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Training\n",
    "    for i in range(epochs):\n",
    "        X_train1 = shuffle(X_train)\n",
    "        for j in range(X_train.shape[0] // batch_size):\n",
    "            batch_y = X_train1[j * batch_size:j * batch_size + batch_size, :]\n",
    "            batch_x = X_train1[j * batch_size:j * batch_size + batch_size, :]\n",
    "            _, loss_value = sess.run([optimizer, mse], feed_dict={X: batch_x, Y: batch_y})\n",
    "\n",
    "        # Display loss\n",
    "        print('Epoch: %i -> Loss: %f' % (i, loss_value))\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred_AE_tf = sess.run(decoder_op, feed_dict={X: X_train, Y: X_train})\n",
    "    print('Test Error: %f' % tf.losses.mean_squared_error(X_train, y_pred_AE_tf).eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
